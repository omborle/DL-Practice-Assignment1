{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c-Gcnh3dOCFe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/dataset.csv'  # Update this path if needed\n",
        "heart_data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "Mp5-PG3cOOYa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values (if any)\n",
        "heart_data.fillna(heart_data.median(), inplace=True)"
      ],
      "metadata": {
        "id": "bdqqhpP-ORqq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting features and target\n",
        "features = heart_data.drop(columns=['target'])\n",
        "target = heart_data['target']\n",
        "\n",
        "# Standardizing numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, target.values, test_size=0.2, random_state=42)\n",
        "\n",
        "# Converting labels to one-hot encoded format\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "y_train_one_hot = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
        "y_test_one_hot = encoder.transform(y_test.reshape(-1, 1)).toarray()"
      ],
      "metadata": {
        "id": "3eVNYpV4ORyc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Parameters\n",
        "input_size = X_train.shape[1]  # Number of features\n",
        "hidden_size = 50  # Hidden layer neurons\n",
        "output_size = y_train_one_hot.shape[1]  # Output classes\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "b2 = np.zeros((1, output_size))"
      ],
      "metadata": {
        "id": "V093FWx3OR04"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation functions and derivatives\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_pred, y_true):\n",
        "    m = y_true.shape[0]\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-8)) / m"
      ],
      "metadata": {
        "id": "GW8SAdlkOR3R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward Pass\n",
        "def forward(X):\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = softmax(z2)\n",
        "    return a1, a2\n",
        "\n",
        "# Backward Pass\n",
        "def backward(X, y, a1, a2):\n",
        "    m = X.shape[0]\n",
        "    dz2 = a2 - y\n",
        "    dW2 = np.dot(a1.T, dz2) / m\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "    dz1 = np.dot(dz2, W2.T) * relu_derivative(a1)\n",
        "    dW1 = np.dot(X.T, dz1) / m\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "    return dW1, db1, dW2, db2"
      ],
      "metadata": {
        "id": "-fgwki2qOR9M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train(X_train, y_train, learning_rate=0.01, epochs=1000):\n",
        "    global W1, b1, W2, b2\n",
        "    for epoch in range(epochs):\n",
        "        a1, a2 = forward(X_train)\n",
        "        loss = cross_entropy_loss(a2, y_train)\n",
        "        dW1, db1, dW2, db2 = backward(X_train, y_train, a1, a2)\n",
        "        W1 -= learning_rate * dW1\n",
        "        b1 -= learning_rate * db1\n",
        "        W2 -= learning_rate * dW2\n",
        "        b2 -= learning_rate * db2\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "# Prediction function\n",
        "def predict(X):\n",
        "    _, a2 = forward(X)\n",
        "    return np.argmax(a2, axis=1)\n"
      ],
      "metadata": {
        "id": "171YgGxWOR_k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train(X_train, y_train_one_hot, learning_rate=0.1, epochs=1000)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_train = predict(X_train)\n",
        "y_pred_test = predict(X_test)\n",
        "\n",
        "y_true_train = np.argmax(y_train_one_hot, axis=1)\n",
        "y_true_test = np.argmax(y_test_one_hot, axis=1)\n",
        "\n",
        "train_accuracy = np.mean(y_pred_train == y_true_train) * 100\n",
        "test_accuracy = np.mean(y_pred_test == y_true_test) * 100\n",
        "\n",
        "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F2r19t9OSCt",
        "outputId": "d59182cc-a82c-4732-bfe2-0bdabb01b438"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/1000, Loss: 0.6934\n",
            "Epoch 100/1000, Loss: 0.3734\n",
            "Epoch 200/1000, Loss: 0.3152\n",
            "Epoch 300/1000, Loss: 0.2959\n",
            "Epoch 400/1000, Loss: 0.2784\n",
            "Epoch 500/1000, Loss: 0.2548\n",
            "Epoch 600/1000, Loss: 0.2274\n",
            "Epoch 700/1000, Loss: 0.2015\n",
            "Epoch 800/1000, Loss: 0.1796\n",
            "Epoch 900/1000, Loss: 0.1601\n",
            "Train Accuracy: 95.45%\n",
            "Test Accuracy: 83.61%\n"
          ]
        }
      ]
    }
  ]
}